{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BQ Cost Optimization Assessment\n",
        "\n",
        "## Summary\n",
        "The purpose of this notebook is to summarize BigQuery pricing for your customer and Google to process SQL queries needed to monitor BigQuery cost and performance.\n",
        "\n",
        "## BigQuery Editions\n",
        "There are 2 pricing strategies for BigQuery\n",
        "On-demand\n",
        "Capacity\n",
        "\n",
        "### On-demand\n",
        "By default, queries are billed using the on-demand (per TiB) pricing model, where you pay for the data scanned by your queries.\n",
        "\n",
        "With on-demand pricing, you will generally have access to up to 2,000 concurrent slots, shared among all queries in a single project. Periodically, BigQuery will temporarily burst beyond this limit to accelerate smaller queries. In addition, you might occasionally have fewer slots available if there is a high amount of contention for on-demand capacity in a specific location.\n",
        "\n",
        "In Europe (eu), the price is $6.25 per TiB.\n",
        "\n",
        "### Capacity\n",
        "BigQuery offers a capacity-based analysis pricing model for customers who prefer a predictable cost for queries rather than paying the on-demand price per TiB of data processed.\n",
        "\n",
        "There are 3 editions *pricing as of October 2023:\n",
        "\n",
        "#### Standard Edition\n",
        "| Commitment model | Hourly cost | Details |\n",
        "| - | - | - |\n",
        "| Pay as you go / autoscale | \\$0.066 / slot hour | Billed per second with a 1 minute minimum |\n",
        "\n",
        "#### Enterprise Edition\n",
        "Commitment model | Hourly cost | Details |\n",
        "| - | - | - |\n",
        "| Pay as you go / autoscale | \\$0.066 / slot hour | Billed per second with a 1 minute minimum |\n",
        "| 1 yr commit | \\$0.0528 / slot hour | Billed for 1 year |\n",
        "| 3 yr commit | \\$0.0396 / slot hour | Billed for 3 years |\n",
        "\n",
        "#### Enterprise Plus Edition\n",
        "Commitment model | Hourly cost | Details |\n",
        "| - | - | - |\n",
        "| Pay as you go / autoscale | \\$0.11 / slot hour | Billed per second with a 1 minute minimum |\n",
        "| 1 yr commit | \\$0.0.88 / slot hour | Billed for 1 year |\n",
        "| 3 yr commit | \\$0.0.66 / slot hour | Billed for 3 years |\n",
        "\n",
        "When deciding between standard and enterprise, it’s important to note there are a few main differences. In the table below, the key advantages of Enterprise are listed. We would advise customers to only use Standard edition for basic ad hoc querying, trials and test projects.\n",
        "\n",
        "\n",
        "\n",
        "Feature | Standard | Enterprise |\n",
        "| - | - | - |\n",
        "| Compute model | Autoscaling | Autoscaling + Baseline |\n",
        "| Maximum reservation size | 1600 slots | None |\n",
        "| VPC Service Controls | None | VPC Service Controls Support |\n",
        "| Data governance\t| None | Column-level access control, Row-level security, Dynamic data masking |\n",
        "| Business Intelligence acceleration | None | BI Engine |\n",
        "| Materialized views | Can query existing Materialized Views | Create materialized views, Automatic refresh of materialized views, Manual refresh of materialized views, Direct query of materialized views, Smart tuning |\n",
        "| Integrated machine learning | None | BiqQuery ML |\n",
        "| Workload management | Users cannot set the maximum concurrency target | Advanced workload management (idle capacity sharing, target concurrency) |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## This Notebook Setup\n",
        "All results will be presented as graphs in this norkbook. It's necessary to first authenticate with Google and also clone the files necessary to output the results. \n",
        "New tables will be built from the INFOMATION_SCHEMA in a chosen GCP project. Note that there are 2 GCP projects identified in the notebook; they could be the same. However, when working with a 3rd party consultant from Google or otherwise, it is often the case that they will have a project set up to help with your cost analysis e.g. `consulant-project` that you give access to all your projects that you want to analyse. Then you can use the consulants project as the CONTROL_PROJECT_NAME. Usually when working on this internally, there is only one project for both CONTROL_PROJECT_NAME and INFOSCHEMA_PROJECT_NAME.\n",
        "\n",
        "- The CONTROL_PROJECT_NAME is the project that will run the queries from this notebook and also the project where the tables we build will be stored under.\n",
        "- The INFOSCHEMA_PROJECT_NAME is the project that we are using for the analysis. The INFOSCHEMA_PROJECT_NAME may have access to multiple projects' INFOMATION_SCHEMA. This can be expected and all accesible projects will appear in the analysis.\n",
        "\n",
        "For example:\n",
        "- There may be a control project called `consulant-project` which doesn't house any data that you use for analytics in your company, however it does have access to all the projects. This is the CONTROL_PROJECT_NAME which is essentially the billing project for the queries we will run below. This is also where the tables for analysis will be created.\n",
        "- Then another project called `bigquery-cost-analysis` which also has access to all the other projects and is the INFOSCHEMA_PROJECT_NAME. This is what will be put in the FROM clause of the INFORMATION_SCEMA. ie `{INFOSCHEMA_PROJECT_NAME}`.`region-eu`.INFORMATION_SCHEMA.JOBS_BY_FOLDER.\n",
        "\n",
        "\n",
        "\n",
        "Please run the next setup code blocks and authenticate as necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Clone files from git\n",
        "! git clone https://github.com/sam-pitcher/bq_cost_optimization_assessment.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title install dependancies from requirements.txt file\n",
        "! pip install -r bq_cost_optimization_assessment/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup variables, please read carefully.\n",
        "\n",
        "# We will build tables used for our analysis in BigQuery.\n",
        "# These tables will need to be built in a project and a dataset.\n",
        "# DESTINATION_PROJECT_NAME is the name of the project you want these tables to be built in.\n",
        "DESTINATION_PROJECT_NAME = 'DESTINATION_PROJECT_NAME'\n",
        "# DESTINATION_DATASET_NAME is the name of the dataset you want these tables to be built in.\n",
        "# Ensure this dataset exists before running the code to build the tables, or you will receive an error.\n",
        "DESTINATION_DATASET_NAME = 'slots_analysis'\n",
        "\n",
        "# CONTROL_PROJECT_NAME is the project that will be querying the INFORMATION_SCHEMA.\n",
        "# This will be used in the Authenication in the next step.\n",
        "CONTROL_PROJECT_NAME = 'CONTROL_PROJECT_NAME'\n",
        "\n",
        "# INFOSCHEMA_PROJECT_NAME is the project that will be in the from clause of the INFORMATION_SCHEMA.\n",
        "# If you have set up a folder where all your projects you want to analyse reside, then set INFOSCHEMA_PROJECT_NAME as a project in that folder and the tables will be built for all projects in the same folder.\n",
        "# You will need to ensure you have the bigquery.jobs.listAll permission at the folder level as either a custom role. Or use roles/bigquery.resourceViewer as this includes bigquery.jobs.listAll.\n",
        "## INFOSCHEMA_PROJECT_NAME = \"my_project\"\n",
        "INFOSCHEMA_PROJECT_NAME = \"YOUR_PROJECT_NAME\"\n",
        "INFOSCHEMA_PROJECT_NAME_FOR_TABLE = INFOSCHEMA_PROJECT_NAME.replace(\"-\", \"_\")\n",
        "# Region for INFOSCHEMA_PROJECT_NAME\n",
        "# Either \"eu\" or \"us\"\n",
        "REGION = \"eu\"\n",
        "# If folders are not set up, then the table will build for a single project. If this is the case, change PROJECT_OR_FOLDER to \"project\"\n",
        "PROJECT_OR_FOLDER = \"folder\"\n",
        "\n",
        "# SQL path for files from git.\n",
        "# If running locally, the SQL_PATH is just \"sql\", otherwise use \"bq_optimization_colab/sql\".\n",
        "# Comment out as appropriate.\n",
        "SQL_PATH = \"sql\"\n",
        "SQL_PATH = \"bq_cost_optimization_assessment/sql\"\n",
        "\n",
        "START_DATE = \"2024-01-01\"\n",
        "END_DATE = \"2024-01-08\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q_rJiXEuR1-A"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "from google.cloud import bigquery\n",
        "import pandas\n",
        "pandas.options.plotting.backend = \"plotly\"\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    from google.colab import data_table\n",
        "    data_table.enable_dataframe_formatter()\n",
        "    auth.authenticate_user()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "client = bigquery.Client(project=CONTROL_PROJECT_NAME, location=REGION)\n",
        "\n",
        "def get_sql(sql_file_path, project_ids=[], time_range=\"\"):\n",
        "    INFO_TABLE_SUFFIX = \"\"\n",
        "    if PROJECT_OR_FOLDER == \"folder\":\n",
        "        INFO_TABLE_SUFFIX = \"_BY_FOLDER\"\n",
        "    if len(project_ids) > 0:\n",
        "        PROJECT_WHERE_CLAUSE = \"AND project_id IN ('\" + \"' , '\".join(project_ids) + \"')\"\n",
        "    else:\n",
        "        PROJECT_WHERE_CLAUSE = \"\"\n",
        "    if len(time_range) > 0:\n",
        "        PERIOD_START_WHERE_CLAUSE = f\"AND TIMESTAMP_TRUNC(period_start, DAY) = TIMESTAMP('{time_range}')\"\n",
        "    else:\n",
        "        PERIOD_START_WHERE_CLAUSE = \"\"\n",
        "    sql = open(f\"{SQL_PATH}/{sql_file_path}\", \"r\").read()\n",
        "    sql = sql.replace(\"{INFO_TABLE_SUFFIX}\", INFO_TABLE_SUFFIX)\n",
        "    sql = sql.replace(\"{INFOSCHEMA_PROJECT_NAME}\", INFOSCHEMA_PROJECT_NAME)\n",
        "    sql = sql.replace(\"{INFOSCHEMA_PROJECT_NAME_FOR_TABLE}\", INFOSCHEMA_PROJECT_NAME_FOR_TABLE)\n",
        "    sql = sql.replace(\"{CONTROL_PROJECT_NAME}\", CONTROL_PROJECT_NAME)\n",
        "    sql = sql.replace(\"{DESTINATION_PROJECT_NAME}\", DESTINATION_PROJECT_NAME)\n",
        "    sql = sql.replace(\"{DESTINATION_DATASET_NAME}\", DESTINATION_DATASET_NAME)\n",
        "    sql = sql.replace(\"{REGION}\", REGION)\n",
        "    # sql = sql.replace(\"{DAYS_AGO}\", f\"{DAYS_AGO}\")\n",
        "    sql = sql.replace(\"{START_DATE}\", f\"{START_DATE}\")\n",
        "    sql = sql.replace(\"{END_DATE}\", f\"{END_DATE}\")\n",
        "    # sql = sql.replace(\"{QUERIES_PER_DAY}\", f\"{QUERIES_PER_DAY}\")\n",
        "    sql = sql.replace(\"{PROJECT_WHERE_CLAUSE}\", PROJECT_WHERE_CLAUSE)\n",
        "    sql = sql.replace(\"{PERIOD_START_WHERE_CLAUSE}\", PERIOD_START_WHERE_CLAUSE)\n",
        "    try:\n",
        "        sql = sql.replace(\"{slot_buckets_sql}\", slot_buckets_case_when_sql)\n",
        "    except:\n",
        "        pass\n",
        "    return sql"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To remove the need of continually hitting the INFORMATION_SCHEMA, it's recommended to build some smaller tables with the necessary information from the INFORMATION_SCHEMA. These tables will then be used throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Build Tables\n",
        "# DAYS_AGO = 30\n",
        "# QUERIES_PER_DAY = 200\n",
        "\n",
        "client.query(get_sql(\"metric_table_builds/job_metrics.sql\"))\n",
        "client.query(get_sql(\"metric_table_builds/job_stages.sql\"))\n",
        "client.query(get_sql(\"metric_table_builds/timeline_metrics.sql\"))\n",
        "client.query(get_sql(\"metric_table_builds/timeline_metrics_by_job.sql\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BigQuery Pricing Plan Assistant\n",
        "BigQuery information schema is holds the essential metadata for your BigQuery projects. It holds details about all your datasets, tables, views, jobs, and more, allowing you to explore and manage your data efficiently.\n",
        "\n",
        "### Steps we need to address:\n",
        "1. Cost effectiveness of measuring slots vs TBs processed\n",
        "2. Compare Standard vs Enterprise\n",
        "3. Compare Enterprise vs Enterprise Plus"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"bq_price_flowchart.png\" width=\"50%\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It’s now important to include metrics about slot usage to understand if capacity pricing is cheaper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title What is the comparison in cost between on-demand and capacity?\n",
        "# Either write a list of project ids as strings, or leave an empty list to include all projects\n",
        "START_DATE = START_DATE #default value is \"2024-01-01\"\n",
        "END_DATE = END_DATE #default value is \"2024-01-08\"\n",
        "\n",
        "PROJECT_IDS = [\"project_1\", \"project_2\"]\n",
        "PROJECT_IDS = []\n",
        "capacity_cost_comparison_df = client.query(get_sql(\"capacity/cost_comparison.sql\", PROJECT_IDS)).to_dataframe()\n",
        "capacity_cost_comparison_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting by project for projects that have >200 average queries per day. By default we have said that projects with under 200 queries per day are classed as ad-hoc, but this can change per customer. You can change the default below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title What is the comparison in cost between on-demand and capacity per project?\n",
        "# Either write a list of project ids as strings, or leave an empty list to include all projects\n",
        "PROJECT_IDS = [\"project_1\", \"project_2\"]\n",
        "PROJECT_IDS = []\n",
        "capacity_cost_comparison_per_project_df = client.query(get_sql(\"capacity/cost_comparison_per_project.sql\", PROJECT_IDS)).to_dataframe()\n",
        "capacity_cost_comparison_per_project_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Are projects using more than 1600 slots?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title What is the consumption and slot usage across projects?\n",
        "# Either write a list of project ids as strings, or leave an empty list to include all projects\n",
        "PROJECT_IDS = [\"project_1\", \"project_2\"]\n",
        "PROJECT_IDS = []\n",
        "percentiles_df = client.query(get_sql(\"capacity/percentiles.sql\", PROJECT_IDS)).to_dataframe()\n",
        "percentiles_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title What is the consumption and slot usage per project?\n",
        "# Either write a list of project ids as strings, or leave an empty list to include all projects\n",
        "PROJECT_IDS = [\"project_1\", \"project_2\"]\n",
        "PROJECT_IDS = []\n",
        "percentiles_per_project_df = client.query(get_sql(\"capacity/percentiles_per_project.sql\", PROJECT_IDS)).to_dataframe()\n",
        "percentiles_per_project_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a heatmap it's easier to see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "percentiles_per_project_df.sort_values(by=['percentile95'], ascending=False).head(20).style.background_gradient(cmap='Blues')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's valuable to see the slots used in buckets of time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title What is the consumption and slot usage in buckets across projects?\n",
        "\n",
        "# Either write a list of project ids as strings, or leave an empty list to include all projects\n",
        "PROJECT_IDS = [\"project_1\", \"project_2\"]\n",
        "PROJECT_IDS = []\n",
        "slot_bucket_maximum = 10000\n",
        "slot_buckets = np.linspace(0,slot_bucket_maximum,11)\n",
        "slot_buckets = np.rint(slot_buckets).tolist()\n",
        "\n",
        "slot_buckets_sort = []\n",
        "slot_buckets_case_when_sql = \"\"\"\n",
        "case\n",
        "\"\"\"\n",
        "for i in range(11):\n",
        "    try:\n",
        "        slot_range = f'{i:02d}. {slot_buckets[i]} < {slot_buckets[i+1]}'\n",
        "        slot_buckets_case_when_sql += f\"\"\"\n",
        "        when total_slots between {slot_buckets[i]} and {slot_buckets[i+1]} then '{slot_range}'\"\"\"\n",
        "    except:\n",
        "        slot_buckets_case_when_sql += f\"\"\"\n",
        "        when total_slots > {slot_buckets[i]} then '{i:02d}. > {slot_buckets[i]}'\n",
        "        end as bucket\n",
        "        \"\"\"\n",
        "    slot_buckets_sort.append(slot_range)\n",
        "\n",
        "slot_buckets_df = client.query(get_sql(\"capacity/slot_buckets.sql\", PROJECT_IDS)).to_dataframe()\n",
        "slot_buckets_per_project_df = client.query(get_sql(\"capacity/slot_buckets_per_project.sql\", PROJECT_IDS)).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slot_buckets_df.style.background_gradient(cmap='Blues', axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slot_buckets_per_project_df.pivot(index='project_id', columns='bucket', values='pct').sort_values(by=slot_buckets_sort[::-1], ascending=False).head(20).style.background_gradient(cmap='Blues', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Either write a list of project ids as strings, or leave an empty list to include all projects\n",
        "PROJECT_IDS = [\"project_1\", \"project_2\"]\n",
        "PROJECT_IDS = []\n",
        "metrics_per_day_df = client.query(get_sql(\"analysis/metrics_per_day.sql\", PROJECT_IDS)).to_dataframe()\n",
        "# metrics_per_day_df.set_index('day', inplace=True)\n",
        "metrics_per_day_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x_axis values: 'day', 'project_id'\n",
        "x_axis = 'day'\n",
        "# y_axis values: 'slot_ms', 'total_query_time', 'total_bytes_processed'\n",
        "y_axis = 'slot_ms'\n",
        "# kind values: 'area', 'bar', 'line'\n",
        "kind = 'area'\n",
        "\n",
        "plot_df = metrics_per_day_df[[x_axis,y_axis]].groupby(x_axis).sum().sort_values(by=[x_axis], ascending=[False])#.head(300)\n",
        "plot_df.plot(kind=kind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Either write a list of project ids as strings, or leave an empty list to include all projects\n",
        "PROJECT_IDS = [\"project_1\", \"project_2\"]\n",
        "PROJECT_IDS = []\n",
        "TIME_RANGE = \"2023-11-04\"\n",
        "metrics_per_second_df = client.query(get_sql(\"analysis/metrics_per_second.sql\", PROJECT_IDS, TIME_RANGE)).to_dataframe()\n",
        "# metrics_per_day_df.set_index('period_start', inplace=True)\n",
        "metrics_per_second_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x_axis values: 'period_start', 'project_id', 'job_id'\n",
        "x_axis = 'period_start'\n",
        "# y_axis values: 'slots', 'total_bytes_processed'\n",
        "y_axis = 'slots'\n",
        "# kind values: 'area', 'bar', 'line'\n",
        "kind = 'area'\n",
        "\n",
        "plot_df = metrics_per_second_df[[x_axis,y_axis]].groupby(x_axis).sum().sort_values(by=[y_axis], ascending=[False])\n",
        "# plot_df.head()\n",
        "plot_df.plot(kind=kind)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BigQuery Slow Query Optimiser\n",
        "BigQuery information schema is holds the essential metadata for your BigQuery jobs. It holds details about all your datasets, tables, views, jobs, and more, allowing you to explore and manage your data efficiently.\n",
        "\n",
        "### Steps we need to identify:\n",
        "1. Slow running queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Either write a list of project ids as strings, or leave an empty list to include all projects\n",
        "PROJECT_IDS = [\"project_1\", \"project_2\"]\n",
        "PROJECT_IDS = []\n",
        "TIME_RANGE = \"2023-11-04\"\n",
        "slow_queries_df = client.query(get_sql(\"queries/slow_queries.sql\", PROJECT_IDS)).to_dataframe()\n",
        "slow_queries_df_top_500 = slow_queries_df.head(500)\n",
        "job_ids = \"'\"+\"','\".join(slow_queries_df_top_500['job_id'].head(50).to_list())+\"'\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slow_queries_df_top_500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT job_id,query FROM `dwh-ingestion-prod`.`region-eu`.INFORMATION_SCHEMA.JOBS\n",
        "WHERE job_id IN\n",
        "({job_ids})\n",
        "\"\"\"\n",
        "slow_queries_df = client.query(query).to_dataframe()\n",
        "slow_queries_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot slowest queries\n",
        "\n",
        "# y_axis values: 'query_duration', 'duration', 'totalSlotUsage', 'peakSlotUsage', 'avgPeriodSlotUsage', 'totalProcessedTb'\n",
        "y_axis = 'query_duration'\n",
        "limit = 30\n",
        "\n",
        "plot_df = slow_queries_df[[\"job_id\",y_axis]].sort_values(by=[y_axis], ascending=[False]).head(limit)\n",
        "fig = px.bar(plot_df, x=\"job_id\", y=y_axis, color=\"job_id\", title=f\"Slowest {limit} queries\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot average value by dimension\n",
        "\n",
        "# x_axis values: 'project_id', 'job_id', 'user_email'\n",
        "x_axis = 'user_email'\n",
        "# y_axis values: 'query_duration', 'duration', 'totalSlotUsage', 'peakSlotUsage', 'avgPeriodSlotUsage', 'totalProcessedTb'\n",
        "y_axis = 'query_duration'\n",
        "limit = 30\n",
        "\n",
        "plot_df = slow_queries_df[[x_axis,y_axis]].groupby(x_axis, as_index=False).mean().sort_values(by=[y_axis], ascending=[False]).head(limit)\n",
        "fig = px.bar(plot_df, x=x_axis, y=y_axis, color=x_axis, title=f\"Average {y_axis} by {x_axis}\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot slowest 500 queries by project and job\n",
        "\n",
        "fig = px.bar(slow_queries_df_top_500, x=\"project_id\", y=\"query_duration\", color=\"job_id\", title=\"Slowest 500 queries by project and job\").show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reservations\n",
        "First, it is important to see the slot usage for each reservation. We can use the pricing above to calculate what edition is best for the reservation below.\n",
        "\n",
        "### Questions for Customer:\n",
        "1. How did you initially distribute projects into reservations?\n",
        "2. Are there specific ELT vs BI vs analytical projects?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4SUa-9QaTf1C"
      },
      "outputs": [],
      "source": [
        "# @title Run query for TB Scanned\n",
        "PROJECT_IDS = []\n",
        "tb_scanned_df = client.query(get_sql(\"tb_scanned/tb_scanned.sql\", PROJECT_IDS)).to_dataframe()\n",
        "tb_scanned_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Run query for TB Scanned per Project\n",
        "PROJECT_IDS = []\n",
        "tb_scanned_per_project_df = client.query(get_sql(\"tb_scanned/tb_scanned_per_project.sql\", PROJECT_IDS)).to_dataframe()\n",
        "tb_scanned_per_project_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Run query for TB Scanned per Reservation\n",
        "PROJECT_IDS = []\n",
        "tb_scanned_per_reservation_df = client.query(get_sql(\"tb_scanned/tb_scanned_per_reservation.sql\", PROJECT_IDS)).to_dataframe()\n",
        "tb_scanned_per_reservation_df.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4166d230a018510500e4e83fb85713ee95bf1dd60ff4576a042122fc8237ae7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
